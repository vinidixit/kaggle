{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Explanation', 'Why', 'the', 'edits', 'made', 'under', 'my', 'username', 'Hardcore', 'Metallica', 'Fan', 'were', 'reverted', '?', 'They', 'were', \"n't\", 'vandalisms', ',', 'just', 'closure', 'on', 'some', 'GAs', 'after', 'I', 'voted', 'at', 'New', 'York', 'Dolls', 'FAC', '.', 'And', 'please', 'do', \"n't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'I', \"'m\", 'retired', 'now.89.205.38.27']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample = train_df['comment_text'][0]\n",
    "words = nltk.word_tokenize(sample)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named contractions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-84dea213c8d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreplace_contractions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named contractions"
     ]
    }
   ],
   "source": [
    "## Tokens preprocessing\n",
    "import re, string, unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "\"\"\"\"\n",
    "import contractions\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\"\"\"\"\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "replace_contractions(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = stem_words(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl cur curl\n",
      "curl curl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "stemmer = LancasterStemmer()\n",
    "print stemmer.stem('curl'),stemmer.stem('curly'),stemmer.stem('curling')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print lemmatizer.lemmatize('curling','v'),lemmatizer.lemmatize('curl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    return stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'clos': 1,\n",
       "         'dol': 1,\n",
       "         'edit': 1,\n",
       "         'expl': 1,\n",
       "         'fac': 1,\n",
       "         'fan': 1,\n",
       "         'gas': 1,\n",
       "         'hardc': 1,\n",
       "         'mad': 1,\n",
       "         'metallic': 1,\n",
       "         'new': 1,\n",
       "         'now892053827': 1,\n",
       "         'nt': 2,\n",
       "         'pag': 1,\n",
       "         'pleas': 1,\n",
       "         'remov': 1,\n",
       "         'retir': 1,\n",
       "         'revert': 1,\n",
       "         u'sint': 1,\n",
       "         'talk': 1,\n",
       "         'templ': 1,\n",
       "         'usernam': 1,\n",
       "         'vand': 1,\n",
       "         'vot': 1,\n",
       "         'york': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-75f95b768cad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print row['comment_text']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'severe_toxic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'severe_toxic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'obscene'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'obscene'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-5bb28bf67274>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_lowercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-4e83fdc6aa0c>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnew_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mnew_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\i309535\\AppData\\Local\\Continuum\\anaconda3\\envs\\py27\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.pyc\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[0;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\i309535\\AppData\\Local\\Continuum\\anaconda3\\envs\\py27\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.pyc\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# features collection\n",
    "import collections\n",
    "classes = [collections.defaultdict(list),collections.defaultdict(list)]\n",
    "#class_feature_distr = collections.defaultdict(list)\n",
    "for ind,row in train_df.iterrows():\n",
    "    #print row['comment_text']\n",
    "    words = preprocess(row['comment_text'])\n",
    "    classes[row['severe_toxic']]['severe_toxic'].extend(words)\n",
    "    classes[row['obscene']]['obscene'].extend(words)\n",
    "    classes[row['threat']]['threat'].extend(words)\n",
    "    classes[row['insult']]['insult'].extend(words)\n",
    "    classes[row['identity_hate']]['identity_hate'].extend(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distr\n",
    "toxic_pos = list(train_df[train_df['toxic']==0].index)\n",
    "toxic_neg = list(train_df[train_df['toxic']==1].index)\n",
    "severe_toxic_pos = list(train_df[train_df['severe_toxic']==0].index)\n",
    "severe_toxic_neg = list(train_df[train_df['severe_toxic']==1].index)\n",
    "obscene_pos = list(train_df[train_df['obscene']==0].index)\n",
    "obscene_neg = list(train_df[train_df['obscene']==1].index)\n",
    "threat_pos = list(train_df[train_df['threat']==0].index)\n",
    "threat_neg = list(train_df[train_df['threat']==1].index)\n",
    "insult_pos = list(train_df[train_df['insult']==0].index)\n",
    "insult_neg = list(train_df[train_df['insult']==1].index)\n",
    "identity_hate_pos = list(train_df[train_df['identity_hate']==0].index)\n",
    "identity_hate_neg = list(train_df[train_df['identity_hate']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test\n",
    "import random\n",
    "def train_test_split(column_name):\n",
    "    pos_indices = list(train_df[train_df[column_name]==0].index)\n",
    "    neg_indices = list(train_df[train_df[column_name]==1].index)\n",
    "    n = int(len(neg_indices)*0.9)\n",
    "    \n",
    "    train_indices = pos_indices[:n]+neg_indices[:n]\n",
    "    test_indices = pos_indices[n:]+neg_indices[n:]\n",
    "\n",
    "    random.shuffle(train_indices)\n",
    "    X_train = train_df.loc[train_indices]['comment_text']\n",
    "    y_train = train_df.loc[train_indices][column_name]\n",
    "    X_test = train_df.loc[test_indices]['comment_text']\n",
    "    y_test = train_df.loc[test_indices][column_name]\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5914\n",
    "pos_indices = list(train_df[train_df['obscene']==0].index)\n",
    "neg_indices = list(train_df[train_df['obscene']==1].index)\n",
    "train_indices = pos_indices[:n]+neg_indices[:n]\n",
    "test_indices = pos_indices[n:]+neg_indices[n:]\n",
    "import random\n",
    "random.shuffle(train_indices)\n",
    "X_train = train_df.loc[train_indices]['comment_text']\n",
    "y_train = train_df.loc[train_indices]['obscene']\n",
    "X_test = train_df.loc[test_indices]['comment_text']\n",
    "y_test = train_df.loc[test_indices]['obscene']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8960940142035847\n",
      "0.937147614438586\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tr_predicted = text_clf.predict(X_train)\n",
    "print np.mean(tr_predicted == y_train) \n",
    "\n",
    "te_predicted = text_clf.predict(X_test)\n",
    "print np.mean(te_predicted == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5625  289]\n",
      " [ 940 4974]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print confusion_matrix(y_train, tr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "def evaluate_classifier(classifier):\n",
    "    text_clf = Pipeline([\n",
    "        ('count',CountVectorizer()),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf',classifier)\n",
    "    ])\n",
    "    \n",
    "    #tfidf_vect = TfidfVectorizer(preprocessor=stem)\n",
    "    #X = tfidf_vect.fit_transform(X_train)\n",
    "    #classifier.fit(X, y_train)\n",
    "    text_clf.fit(X_train, y_train) \n",
    "    tr_predicted = text_clf.predict(X_train)\n",
    "    print 'Training:'\n",
    "    print np.mean(tr_predicted == y_train) \n",
    "    print roc_auc_score(y_train, tr_predicted)\n",
    "    print confusion_matrix(y_train, tr_predicted),'\\n'\n",
    "            \n",
    "    te_predicted = text_clf.predict(X_test)\n",
    "    print 'Test:'\n",
    "    print np.mean(te_predicted == y_test) \n",
    "    print roc_auc_score(y_test, te_predicted)\n",
    "    print confusion_matrix(y_test, te_predicted),'\\n'  \n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.9413256679066622\n",
      "0.9413256679066622\n",
      "[[5697  217]\n",
      " [ 477 5437]] \n",
      "\n",
      "Test:\n",
      "0.9378650765180077\n",
      "0.9071508494867241\n",
      "[[136344   8864]\n",
      " [   316   2219]] \n",
      "\n",
      "Training:\n",
      "0.995434562056138\n",
      "0.995434562056138\n",
      "[[5875   39]\n",
      " [  15 5899]] \n",
      "\n",
      "Test:\n",
      "0.9456624002490812\n",
      "0.9301095146707594\n",
      "[[137398   7810]\n",
      " [   218   2317]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clf = evaluate_classifier(SGDClassifier())\n",
    "clf_log = evaluate_classifier(SGDClassifier(loss='log'))\n",
    "clf_mod_huber = evaluate_classifier(SGDClassifier(loss='modified_huber'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comment = 'Yo bitch Ja Rule is more succesful then you will ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da'\n",
    "clf_mod_huber.predict_proba([test_comment])\n",
    "clf_mod_huber.predict([test_comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.9797937098410551\n",
      "0.9797937098410551\n",
      "[[5813  101]\n",
      " [ 138 5776]] \n",
      "\n",
      "Test:\n",
      "0.9524715214934041\n",
      "0.9283410374420935\n",
      "[[138431   6777]\n",
      " [   245   2290]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('count', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        s...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC,SVC\n",
    "evaluate_classifier(SVC(kernel='linear',probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.9779542269967305\n",
      "0.9779542269967304\n",
      "[[10457   248]\n",
      " [  224 10481]] \n",
      "\n",
      "Test:\n",
      "0.9125440609144404\n",
      "0.8985860235543381\n",
      "[[122023  11549]\n",
      " [   534   4055]] \n",
      "\n",
      "Training:\n",
      "0.9982078853046595\n",
      "0.9982078853046594\n",
      "[[1115    1]\n",
      " [   3 1113]] \n",
      "\n",
      "Test:\n",
      "0.9155708374910225\n",
      "0.9097864350935118\n",
      "[[143622  13238]\n",
      " [    46    433]] \n",
      "\n",
      "Training:\n",
      "0.9955191072032465\n",
      "0.9955191072032465\n",
      "[[5885   29]\n",
      " [  24 5890]] \n",
      "\n",
      "Test:\n",
      "0.9567221458884685\n",
      "0.9303096574136949\n",
      "[[139060   6148]\n",
      " [   246   2289]] \n",
      "\n",
      "Training:\n",
      "1.0\n",
      "1.0\n",
      "[[334   0]\n",
      " [  0 334]] \n",
      "\n",
      "Test:\n",
      "0.8893853482942424\n",
      "0.8926064165391149\n",
      "[[141197  17562]\n",
      " [    15    129]] \n",
      "\n",
      "Training:\n",
      "0.99383275893343\n",
      "0.99383275893343\n",
      "[[5462   51]\n",
      " [  17 5496]] \n",
      "\n",
      "Test:\n",
      "0.9277121411020229\n",
      "0.9137472104067355\n",
      "[[135681  10500]\n",
      " [   238   2126]] \n",
      "\n",
      "Training:\n",
      "0.9989827060020345\n",
      "0.9989827060020346\n",
      "[[983   0]\n",
      " [  2 981]] \n",
      "\n",
      "Test:\n",
      "0.8819263348244027\n",
      "0.8888119299950826\n",
      "[[138618  18565]\n",
      " [    44    378]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(2,8):\n",
    "    col_name = train_df.columns[i]\n",
    "    X_train,y_train,X_test,y_test = train_test_split(col_name)\n",
    "    clf = evaluate_classifier(SGDClassifier(loss='modified_huber'))\n",
    "    models.append((col_name,clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv',encoding='utf-8')\n",
    "columns = ['id']+list(train_df.columns[2:8])\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "results_df['id'] = test_df['id']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname,model in models:\n",
    "    res = model.predict_proba(test_df['comment_text'])\n",
    "    results_df[colname] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.743326</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.765029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982901</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene  threat    insult  \\\n",
       "0  00001cee341fdb12  0.000000      0.000000  0.000000     0.0  0.000000   \n",
       "1  0000247867823ef7  1.000000      0.823851  1.000000     1.0  1.000000   \n",
       "2  00013b17ad220c46  1.000000      1.000000  0.891873     1.0  0.743326   \n",
       "3  00017563c3f7919a  1.000000      1.000000  1.000000     1.0  1.000000   \n",
       "4  00017695ad8997eb  0.765029      1.000000  0.908508     1.0  0.982901   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.000000  \n",
       "1       1.000000  \n",
       "2       1.000000  \n",
       "3       0.949218  \n",
       "4       1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('test_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=15000, min_df=1,\n",
       "        ngram_range=(1, 2), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = train_df['comment_text']\n",
    "vectoriser = TfidfVectorizer(sublinear_tf=True,\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=15000)\n",
    "vectoriser.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'comment_text', u'toxic', u'severe_toxic', u'obscene',\n",
       "       u'threat', u'insult', u'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.9603824160521857\n",
      "test score accuracy 0.9334307763380111 0.9007630246835402 \n",
      "\n",
      "severe_toxic 0.982333297970963\n",
      "test score accuracy 0.9688642701705796 0.915736492356635 \n",
      "\n",
      "obscene 0.9805624341456918\n",
      "test score accuracy 0.9714123424977315 0.9179753219005331 \n",
      "\n",
      "threat 0.973149138328577\n",
      "test score accuracy 0.9708715842002129 0.9021232633527246 \n",
      "\n",
      "insult 0.969496111557146\n",
      "test score accuracy 0.9526455881644921 0.9074082214511543 \n",
      "\n",
      "identity_hate 0.9682494511404242\n",
      "test score accuracy 0.9532548410308005 0.9092910775674132 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "colnames = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for colname in colnames:\n",
    "    X_train,train_target,X_test,y_test = train_test_split(colname)\n",
    "    #X_train,train_target = train_df['comment_text'], train_df[colname]\n",
    "    \n",
    "    train_features = vectoriser.transform(X_train)\n",
    "    classifier = LogisticRegression(solver='sag')#SGDClassifier(loss='log')\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    print colname, cv_score\n",
    "    classifier.fit(train_features,train_target)\n",
    "    test_features = vectoriser.transform(X_test)\n",
    "    te_predicted = classifier.predict(test_features)\n",
    "    print 'test score accuracy', np.mean(te_predicted == y_test),roc_auc_score(y_test, te_predicted),'\\n'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.9693287995396691\n",
      "severe_toxic 0.9860163108666002\n",
      "obscene 0.985220193390691\n",
      "threat 0.9823253412465484\n",
      "insult 0.976192055968359\n",
      "identity_hate 0.9743973759002674\n"
     ]
    }
   ],
   "source": [
    "for colname in colnames:\n",
    "    X_train,train_target = train_df['comment_text'], train_df[colname]\n",
    "    \n",
    "    train_features = vectoriser.transform(X_train)\n",
    "    classifier = LogisticRegression(solver='sag')#SGDClassifier(loss='log')\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    print colname, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic 0.9694377089374315\n",
    "severe_toxic 0.9835987922361634\n",
    "obscene 0.9825913759390813\n",
    "threat 0.9827427709355318\n",
    "insult 0.9749755607373317\n",
    "identity_hate 0.9722185220302423\n",
    "\n",
    "\n",
    "toxic 0.9585138739296148\n",
    "test score accuracy 0.9240550426754921 0.9024795291221083\n",
    "severe_toxic 0.9794910664963711\n",
    "test score accuracy 0.9624188741616199 0.9125105004439732\n",
    "obscene 0.9744300964732768\n",
    "test score accuracy 0.9611742621031705 0.9187084629344887\n",
    "threat 0.9691597213480713\n",
    "test score accuracy 0.949638021309172 0.8915032700335513\n",
    "insult 0.9663699077331698\n",
    "test score accuracy 0.9378787149312553 0.9062951507844668\n",
    "identity_hate 0.9585100649984408\n",
    "test score accuracy 0.9355335799749114 0.9145941347858549"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
